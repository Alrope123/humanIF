[build-system]
requires = ["setuptools>=42", "wheel", "setuptools_scm"]
build-backend = "setuptools.build_meta"

[project]
name = "href"
version = "0.1.0"
description = "Human Reference Guided Evaluation for Instruction Following"
readme = "README.md"
authors = [
    { name = "Xinxi Lyu", email = "xinxil@allenai.org" }
]
requires-python = ">=3.10"
dependencies = [
    # Git requirement
    "alpaca-eval @ git+https://github.com/Alrope123/alpaca_eval.git#egg=alpaca-eval",
    # Standard packages
    "datasets",
    "openai",
    "asyncio",
    "tqdm",
    "numpy",
    "transformers==4.46.1",
    "scipy",
    "bert-score",
    "rouge",
    "torch==2.4.0",
    "vllm>=0.6.0",
    "accelerate>=0.26.0",
    "bitsandbytes>=0.41.1",
    "peft>=0.4.0",
    "seaborn",
    "matplotlib",
    "pandas"
]
license = { text = "" } # Replace this with the license name or file as needed
keywords = ["evaluation", "instruction-following", "NLP"]

[tool.setuptools.packages.find]
include = ["href*"]
exclude = ["tests*", "docs*"]

[project.scripts]
evaluate = "href.evaluation:evaluate.main"
calculate_agreement = "href.evaluation.calculate_human_agreement_rate:main"
create_config = "href.llm_as_a_judge.create_config:main"
